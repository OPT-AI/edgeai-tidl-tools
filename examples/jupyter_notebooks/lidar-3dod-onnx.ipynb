{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Point Cloud Based 3D Object Detection Inference using ONNX Runtime\n",
    "\n",
    "\n",
    "## Inference using Pre-Compiled Models\n",
    "In this example notebook, we describe how to use a pre-trained Point Cloud Based 3D Object Detection model for inference using the ***ONNX Runtime interface***.\n",
    "   - Model is based on point pillars [Link](https://arxiv.org/abs/1812.05784) method.\n",
    "   - Model is trained for only one class, which is 'car'\n",
    "   - We perform inference on a few sample point clouds\n",
    "   - We also describe the input preprocessing and output postprocessing steps, demonstrate how to collect various benchmarking statistics and how to visualize the data.\n",
    "\n",
    "\n",
    "## Point Cloud Based 3D Object Detection  \n",
    "\n",
    "3D Object Detection is a popular computer vision algorithm used in many applications such as Person Detection and Vehicle detection. 3D object information provides better understanding surrounding and hence helps in precise path planning. 3D object detection on lidar data outperforms than image data. \n",
    "\n",
    "## ONNX Runtime based Work flow\n",
    "\n",
    "The diagram below describes the steps for ONNX Runtime based workflow. \n",
    "\n",
    "Note:\n",
    "- The user needs to compile models(sub-graph creation and quantization) on a PC to generate model artifacts.\n",
    "    - For this notebook we use pre-compiled models artifacts\n",
    "- The generated artifacts can then be used to run inference on the target.\n",
    "- Users can run this notebook as-is, only action required is to select a model. \n",
    "\n",
    "<img src=docs/images/onnx_work_flow_2.png width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from scripts.utils import get_eval_configs\n",
    "\n",
    "prebuilt_configs, selected_model_id = get_eval_configs('detection_3d','onnxrt', \n",
    "                                                       num_quant_bits = 8, \n",
    "                                                       last_artifacts_id = None,\n",
    "                                                       experimental_models=True)\n",
    "display(selected_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Selected Model: {selected_model_id.label}')\n",
    "config = prebuilt_configs[selected_model_id.value]\n",
    "config['session'].set_param('model_id', selected_model_id.value)\n",
    "config['session'].start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize configuration parameter required for point cloud processing\n",
    "\n",
    "Input point cloud is segregated in 3D bins called as voxels.Features are computed for each voxels. Steps involved in pre-processing are\n",
    "\n",
    " 1. Initialize the voxel property such as min_x, max_x.... voxel_size_x, voxel_size_y etc.\n",
    " 2. Initialize the network related limitations such as max_points_per_voxel, nw_max_num_voxels, num_feat_per_voxel.\n",
    " 3. Point clouds are which are outside the interested range are discarded\n",
    " 4. Point cloud is segregated in voxels\n",
    " 5. Features are computed for each voxel as per the model requirement.\n",
    " \n",
    " step #1 and #2 happens in below cell\n",
    " step #3 to #5 happens as part of preprocessing API 'voxelization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxelization_config_params={}\n",
    "\n",
    "# voxel property \n",
    "voxelization_config_params['min_x'] = 0\n",
    "voxelization_config_params['max_x'] = 69.120\n",
    "voxelization_config_params['min_y'] = -39.680\n",
    "voxelization_config_params['max_y'] = 39.680\n",
    "voxelization_config_params['min_z'] = -3.0\n",
    "voxelization_config_params['max_z'] = 1.0\n",
    "voxelization_config_params['voxel_size_x']= 0.16\n",
    "voxelization_config_params['voxel_size_y']= 0.16\n",
    "voxelization_config_params['num_voxel_x'] = (voxelization_config_params['max_x'] - voxelization_config_params['min_x'])/voxelization_config_params['voxel_size_x']\n",
    "voxelization_config_params['num_voxel_y'] = (voxelization_config_params['max_y'] - voxelization_config_params['min_y'])/voxelization_config_params['voxel_size_y']\n",
    "\n",
    "# network property has to align with below parameters\n",
    "voxelization_config_params['max_points_per_voxel'] = 32\n",
    "voxelization_config_params['nw_max_num_voxels']    = 20000\n",
    "voxelization_config_params['num_feat_per_voxel']   = 9\n",
    "voxelization_config_params['num_channels']         = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model using the stored artifacts\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b> It is recommended to use the ONNX Runtime APIs in the cells below without any modifications.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "onnx_model_path = config['session'].get_param('model_file')\n",
    "delegate_options = {}\n",
    "so = rt.SessionOptions()\n",
    "delegate_options['artifacts_folder'] = config['session'].get_param('artifacts_folder')\n",
    "\n",
    "EP_list = ['TIDLExecutionProvider','CPUExecutionProvider']\n",
    "sess = rt.InferenceSession(onnx_model_path ,providers=EP_list, provider_options=[delegate_options, {}], sess_options=so)\n",
    "\n",
    "input_details = sess.get_inputs()\n",
    "output_details = sess.get_outputs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Point Cloud data\n",
    "Prepare the list of point cloud data to be processed and similar list has to prepared for image data and clibration data.\n",
    "    \n",
    "\n",
    "### Disclaimer ::\n",
    "  - We use one sample point cloud data, corresponding image data and calibration information from https://github.com/azureology/kitti-velo2cam/blob/master/readme.md. Currently only one lidar frame is hosted there.\n",
    "  - This sample point cloud is from Kitti data set, and user of this jupyter notebook is assumed to have agreed to all the terms and conditions for usages of this dataset content. \n",
    "  - Refer \"http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d\" for license of usages for Kitti 3d-od dataset.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "image_files = [\n",
    "    ('000007.png')\n",
    "]\n",
    "\n",
    "point_cloud_files =[\n",
    "    ('000007.bin'),\n",
    "]\n",
    "\n",
    "calib_files =[\n",
    "    ('000007.txt'),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform inference\n",
    "\n",
    "### Preprocessing and Inference\n",
    "  - We perform inference on a set of lidar data mentioned in the list 'point_cloud_files'. \n",
    "  - We use a loop to preprocess the selected lidar point cloud, and provide them as the input to the network.\n",
    "\n",
    "### Postprocessing and Visualization\n",
    " - Once the inference results are available, we postpocess the results and visualize the inferred classes for each of the input lidar data.\n",
    " - Object Detection models return results as a list (i.e. `numpy.ndarray`) with length of 9. Each element in this list contains, the detected object class ID, the probability of the detection and the bounding box co-ordinates.\n",
    " - We use the `boxes3d_to_corners3d_lidar()` function to draw detected 3D boxes on the each object with unique colors for each class ID.\n",
    " - Then, in this notebook, we use *matplotlib* to plot the original images and the corresponding results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import urllib\n",
    "\n",
    "from scripts.utils_lidar import boxes3d_to_corners3d_lidar\n",
    "from scripts.utils_lidar import draw_lidar_bbox3d_on_img\n",
    "from scripts.utils_lidar import voxelization\n",
    "from scripts.utils_lidar import align_img_and_pc\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "# use results from the past inferences\n",
    "input0 = np.zeros((1, \n",
    "                   voxelization_config_params['num_feat_per_voxel'], \n",
    "                   voxelization_config_params['max_points_per_voxel'], \n",
    "                   voxelization_config_params['nw_max_num_voxels']),\n",
    "                  dtype='float32')\n",
    "\n",
    "input1 = np.zeros((1, voxelization_config_params['num_channels'], \n",
    "                   (int)(voxelization_config_params['num_voxel_x']*voxelization_config_params['num_voxel_y'])),\n",
    "                  dtype='float32')\n",
    "\n",
    "input2 = np.zeros((1, voxelization_config_params['num_channels'], \n",
    "                   voxelization_config_params['nw_max_num_voxels']),\n",
    "                  dtype='int32')\n",
    "ax = []\n",
    "\n",
    "for num in tqdm.trange(len(image_files)):\n",
    "    image_file = image_files[num]\n",
    "    point_cloud_file = point_cloud_files[num]\n",
    "    calib_file       = calib_files[num]\n",
    "\n",
    "    # read int8 image data\n",
    "    req = urllib.request.urlopen('https://raw.githubusercontent.com/azureology/kitti-velo2cam/master/data_object_image_2/testing/image_2/' + image_file)\n",
    "    arr = np.asarray(bytearray(req.read()), dtype=np.int8)\n",
    "    img = cv2.imdecode(arr, -1) # 'Load it as it is'\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # read point float32 cloud data\n",
    "    req = urllib.request.urlopen('https://raw.githubusercontent.com/azureology/kitti-velo2cam/master/data_object_velodyne/testing/velodyne/' + point_cloud_file)\n",
    "    arr = np.fromstring(req.read(), dtype=np.float32).reshape(-1, 4)\n",
    "\n",
    "    # read calib text data\n",
    "    req = urllib.request.urlopen('https://raw.githubusercontent.com/azureology/kitti-velo2cam/master/testing/calib/' + calib_file)\n",
    "    decoded_line = []\n",
    "    for line in req:\n",
    "        decoded_line.append(line.decode(\"utf-8\"))\n",
    "\n",
    "    # point cloud data will have data from all around 360 degree, hence consider the point cloud \n",
    "    #which are in front of camera view\n",
    "    lidar_data, lidar2img_rt = align_img_and_pc(img, arr, decoded_line)\n",
    "\n",
    "    voxelization(lidar_data,voxelization_config_params,input0[0],input2[0][0])\n",
    "    input2[0][1:64] = input2[0][0]\n",
    "\n",
    "    output = sess.run(None, {input_details[0].name: input0, input_details[1].name: input2, input_details[2].name: input1})\n",
    "    selected_objs = output[0][0][0][(output[0][0][0][:,1]>0.4)]\n",
    "    corners = boxes3d_to_corners3d_lidar(selected_objs[:,2:])\n",
    "    img = draw_lidar_bbox3d_on_img(corners,img,lidar2img_rt)\n",
    "    \n",
    "    ax.append(fig.add_subplot(len(image_files), 1, num+1) )\n",
    "    plt.imshow(img)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Inference benchmarking statistics\n",
    " - During model execution several benchmarking statistics such as timestamps at different checkpoints, DDR bandwidth are collected and stored. \n",
    " - The `get_TI_benchmark_data()` function can be used to collect these statistics. The statistics are collected as a dictionary of `annotations` and corresponding markers.\n",
    " - We provide the utility function plot_TI_benchmark_data to visualize these benchmark KPIs.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> The values represented by <i>Inferences Per Second</i> and <i>Inference Time Per Image</i> uses the total time taken by the inference except the time taken for copying inputs and outputs. In a performance oriented system, these operations can be bypassed by writing the data directly into shared memory and performing on-the-fly input / output normalization.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import plot_TI_performance_data, plot_TI_DDRBW_data, get_benchmark_output\n",
    "stats = sess.get_TI_benchmark_data()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5))\n",
    "plot_TI_performance_data(stats, axis=ax)\n",
    "plt.show()\n",
    "\n",
    "tt, st, rb, wb = get_benchmark_output(stats)\n",
    "\n",
    "print(f'SoC: J721E/DRA829/TDA4VM')\n",
    "print(f' OPP:')\n",
    "print(f'  Cortex-A72 @2GHZ')\n",
    "print(f'  DSP C7x-MMA @1GHZ')\n",
    "print(f'  DDR @4266 MT/s\\n')\n",
    "print(f'{selected_model_id.label} :')\n",
    "print(f' Inferences Per Second    : {1000.0/tt :7.2f} fps')\n",
    "print(f' Inference Time Per Image : {tt :7.2f} ms')\n",
    "print(f' DDR usage Per Image      : {rb+ wb : 7.2f} MB')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
